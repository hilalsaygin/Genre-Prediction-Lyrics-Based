{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from gensim.downloader import load as gensim_load\n",
    "# import spacy  # Uncomment if spaCy is used\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sparse Matrix Operations\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction\n",
    "The dataset originates from the Hugging Face genius-song-lyrics repository, which serves as the initial source of song lyrics. To further enhance the dataset and ensure a more comprehensive collection, additional lyrics were scraped directly from Genius.com, a widely recognized platform that hosts lyrics across various genres.\n",
    "In final version, the dataset encompasses a diverse range of musical styles, including rock, rap, pop, country, and R&B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "country    24766\n",
       "pop        24789\n",
       "rap        24862\n",
       "rb         24828\n",
       "rock       24846\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/uni.csv')\n",
    "df=df.drop_duplicates(subset='lyrics', keep='first', ignore_index=False)\n",
    "df.groupby('tag').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "country    10000\n",
       "pop        10000\n",
       "rap        10000\n",
       "rb         10000\n",
       "rock       10000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Training dataset 10k songs for each genre\n",
    "df = pd.read_csv('./data/uniform.csv') \n",
    "df=df.drop_duplicates(subset='lyrics', keep='first', ignore_index=False)\n",
    "df.groupby('tag').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train_samp.csv')\n",
    "#split lyrics into chorus and non-chorus subsets\n",
    "df=df.drop_duplicates(subset='lyrics', keep='first', ignore_index=False)\n",
    "chorus_data = []\n",
    "non_chorus = []\n",
    "for idx, row in df.iterrows():\n",
    "    rlyr = row['lyrics']\n",
    "    genre = row['tag']\n",
    "    sections = re.split(r'\\[(.*?)\\]', rlyr)\n",
    "    if sections[0].strip():\n",
    "        content = sections[0].strip()\n",
    "        data = {\"tag\": genre, \"lyrics\": content}\n",
    "        chorus_data.append(data)  #unlabed as chorus\n",
    "        \n",
    "    for i in range(2, len(sections), 2):\n",
    "        if sections[i-1] == '' or sections[i] =='':\n",
    "            continue\n",
    "        section_type = sections[i-1].strip().lower()\n",
    "        content = sections[i].strip()\n",
    "        if content:\n",
    "            data = {\"tag\":genre,\"lyrics\":content}\n",
    "            # Handle chorus sections\n",
    "            if 'chor' in section_type:\n",
    "                chorus_data.append(data)\n",
    "                \n",
    "            else:\n",
    "                non_chorus.append(data)\n",
    "        \n",
    "\n",
    "chorus_df = pd.DataFrame(chorus_data)\n",
    "non_chorus_df = pd.DataFrame(non_chorus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "class LyricsPreprocessor:\n",
    "    def __init__(self, custom_stopwords=None):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor with optional custom stopwords.\n",
    "        \n",
    "        Args:\n",
    "            custom_stopwords (list): Additional stopwords specific to lyrics\n",
    "        \"\"\"\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        if custom_stopwords:\n",
    "            # Add custom stopwords for lyrics\n",
    "            self.custom_stopwords = set(custom_stopwords or [])\n",
    "            self.stop_words.update(self.custom_stopwords)\n",
    "    \n",
    "    def expand_contractions(self, text):\n",
    "        \"\"\"Expand contractions like \"I'm\" to \"I am\".\"\"\"\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    def only_remove_non_alphanum(self, text):\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\']', ' ', text)\n",
    "        # Remove standalone apostrophes\n",
    "        text = re.sub(r'\\s\\'|\\'\\s', ' ', text)\n",
    "        return self.remove_extra_whitespace(text)\n",
    "    \n",
    "    def remove_special_chars(self, text):\n",
    "        \"\"\"Remove special characters and digits.\"\"\"\n",
    "        # Keep apostrophes for contractions but remove other special chars\n",
    "        text = re.sub(r'[^a-zA-Z\\s\\']', ' ', text)\n",
    "        # Remove standalone apostrophes\n",
    "        text = re.sub(r'\\s\\'|\\'\\s', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_extra_whitespace(self, text):\n",
    "        \"\"\"Remove extra whitespace and newlines.\"\"\"\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        \"\"\"Lemmatize words to their root form.\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in words])\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove common stopwords.\"\"\"\n",
    "        words = text.split()\n",
    "        return ' '.join([word for word in words if word.lower() not in self.stop_words])\n",
    "    \n",
    "    def preprocess_text(self, text, steps=None):\n",
    "        if steps is None:\n",
    "            steps = ['expand_contractions', 'remove_special_chars', \n",
    "                    'remove_extra_whitespace', 'lemmatize_text', 'remove_stopwords']\n",
    "        \n",
    "        # Convert to lowercase first\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        for step in steps:\n",
    "            if hasattr(self, step):\n",
    "                text = getattr(self, step)(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df, column='lyrics', steps=None, inplace=False):\n",
    "\n",
    "        if not inplace:\n",
    "            df = df.copy()\n",
    "        \n",
    "        # Add preprocessed column\n",
    "        preprocessed_column = f'preprocessed_{column}'\n",
    "        df[preprocessed_column] = df[column].apply(lambda x: self.preprocess_text(x, steps))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    return LyricsPreprocessor().preprocess_dataframe(chorus_df)\n",
    "ch_processed = preprocess_df(chorus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test single subset df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.614614191000141\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.61      0.53      0.57      1316\n",
      "         pop       0.75      0.29      0.42       914\n",
      "         rap       0.68      0.79      0.74      1901\n",
      "          rb       0.53      0.75      0.62      1831\n",
      "        rock       0.64      0.46      0.54      1127\n",
      "\n",
      "    accuracy                           0.61      7089\n",
      "   macro avg       0.64      0.56      0.57      7089\n",
      "weighted avg       0.63      0.61      0.60      7089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import vstack\n",
    "def test(model, X_test, y_test):\n",
    "    # Step 6: Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_test, y_pred)*100, 2))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "   \n",
    "def tfidf_weighted_combined(df1, df2, text_column='lyrics', weight1=1.0, weight2=0.4):\n",
    "    # Fill missing values with empty strings and ensure all data is string\n",
    "    df1[text_column] = df1[text_column].fillna('').astype(str)\n",
    "    df2[text_column] = df2[text_column].fillna('').astype(str)\n",
    "\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1, 3), dtype=np.float32)\n",
    "\n",
    "    # Transform chorus and non-chorus parts separately\n",
    "    chorus_features = tfidf.fit_transform(df1[text_column]) * weight1\n",
    "    non_chorus_features = tfidf.transform(df2[text_column]) * weight2\n",
    "\n",
    "    # Combine the features\n",
    "    combined_features = vstack([chorus_features, non_chorus_features])\n",
    "\n",
    "    # Combine target labels\n",
    "    combined_labels = np.concatenate([df1['tag'].values, df2['tag'].values])\n",
    "\n",
    "    return combined_features, combined_labels, tfidf\n",
    "\n",
    "\n",
    "\n",
    "def extract_and_reduce_features(df1, df2, weight1=1.0, weight2=1.0):\n",
    "    # Get preprocessed features\n",
    "    X_combined, y_combined, tfidf = tfidf_weighted_combined(\n",
    "        df1, df2, weight1=weight1, weight2=weight2\n",
    "    )\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    svd = TruncatedSVD(n_components=400, random_state=42)\n",
    "    X_combined = svd.fit_transform(X_combined)\n",
    "    return X_combined, y_combined, tfidf, svd\n",
    "\n",
    "def train_weighted_pipeline(df1, df2, model, weight1=1.0, weight2=0.8, model_name='weighted_model'):\n",
    "    # Get preprocessed features\n",
    "    \n",
    "    X_reduced , y_combined, tfidf,svd = extract_and_reduce_features(df1,df2,weight1,weight2)\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reduced, y_combined, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', model)  # Classifier only\n",
    "    ])\n",
    "\n",
    "    # Train the classifier\n",
    "    pipeline.named_steps['classifier'].fit(X_train, y_train)\n",
    "\n",
    "    # Save the pipeline with tfidf and svd as additional components\n",
    "    dump({'pipeline': pipeline, 'tfidf': tfidf, 'svd': svd}, model_name + '_pipeline.joblib')\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "# X_reduced, y_combined, tfidf, svd = extract_features(chorus_df, non_chorus_df,weight1=0.9, weight2=0.6)\n",
    "# # Split into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_reduced, y_combined, test_size=0.1, random_state=42\n",
    "# )\n",
    "\n",
    "# test(load(\"w-rf-pip_pipeline.joblib\"),X_test, y_test)\n",
    "train_weighted_pipeline(chorus_df, non_chorus_df, RandomForestClassifier( n_jobs=-1), \n",
    "                     weight1=1.5, weight2=0.9, model_name=\"4k-svd4-w\")"
   ]
  },
  
 "metadata": {
  "kernelspec": {
   "display_name": "nb_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
