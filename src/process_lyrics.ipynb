{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from gensim.downloader import load as gensim_load\n",
    "# import spacy  # Uncomment if spaCy is used\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sparse Matrix Operations\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction\n",
    "The dataset originates from the Hugging Face genius-song-lyrics repository, which serves as the initial source of song lyrics. To further enhance the dataset and ensure a more comprehensive collection, additional lyrics were scraped directly from Genius.com, a widely recognized platform that hosts lyrics across various genres.\n",
    "In final version, the dataset encompasses a diverse range of musical styles, including rock, rap, pop, country, and R&B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "country    24766\n",
       "pop        24789\n",
       "rap        24862\n",
       "rb         24828\n",
       "rock       24846\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/uni.csv')\n",
    "df=df.drop_duplicates(subset='lyrics', keep='first', ignore_index=False)\n",
    "df.groupby('tag').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "country    10000\n",
       "pop        10000\n",
       "rap        10000\n",
       "rb         10000\n",
       "rock       10000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Training dataset 10k songs for each genre\n",
    "df = pd.read_csv('./data/uniform.csv') \n",
    "df=df.drop_duplicates(subset='lyrics', keep='first', ignore_index=False)\n",
    "df.groupby('tag').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train_samp.csv')\n",
    "#split lyrics into chorus and non-chorus subsets\n",
    "df=df.drop_duplicates(subset='lyrics', keep='first', ignore_index=False)\n",
    "chorus_data = []\n",
    "non_chorus = []\n",
    "for idx, row in df.iterrows():\n",
    "    rlyr = row['lyrics']\n",
    "    genre = row['tag']\n",
    "    sections = re.split(r'\\[(.*?)\\]', rlyr)\n",
    "    if sections[0].strip():\n",
    "        content = sections[0].strip()\n",
    "        data = {\"tag\": genre, \"lyrics\": content}\n",
    "        chorus_data.append(data)  #unlabed as chorus\n",
    "        \n",
    "    for i in range(2, len(sections), 2):\n",
    "        if sections[i-1] == '' or sections[i] =='':\n",
    "            continue\n",
    "        section_type = sections[i-1].strip().lower()\n",
    "        content = sections[i].strip()\n",
    "        if content:\n",
    "            data = {\"tag\":genre,\"lyrics\":content}\n",
    "            # Handle chorus sections\n",
    "            if 'chor' in section_type:\n",
    "                chorus_data.append(data)\n",
    "                \n",
    "            else:\n",
    "                non_chorus.append(data)\n",
    "        \n",
    "\n",
    "chorus_df = pd.DataFrame(chorus_data)\n",
    "non_chorus_df = pd.DataFrame(non_chorus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "class LyricsPreprocessor:\n",
    "    def __init__(self, custom_stopwords=None):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor with optional custom stopwords.\n",
    "        \n",
    "        Args:\n",
    "            custom_stopwords (list): Additional stopwords specific to lyrics\n",
    "        \"\"\"\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        if custom_stopwords:\n",
    "            # Add custom stopwords for lyrics\n",
    "            self.custom_stopwords = set(custom_stopwords or [])\n",
    "            self.stop_words.update(self.custom_stopwords)\n",
    "    \n",
    "    def expand_contractions(self, text):\n",
    "        \"\"\"Expand contractions like \"I'm\" to \"I am\".\"\"\"\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    def only_remove_non_alphanum(self, text):\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\']', ' ', text)\n",
    "        # Remove standalone apostrophes\n",
    "        text = re.sub(r'\\s\\'|\\'\\s', ' ', text)\n",
    "        return self.remove_extra_whitespace(text)\n",
    "    \n",
    "    def remove_special_chars(self, text):\n",
    "        \"\"\"Remove special characters and digits.\"\"\"\n",
    "        # Keep apostrophes for contractions but remove other special chars\n",
    "        text = re.sub(r'[^a-zA-Z\\s\\']', ' ', text)\n",
    "        # Remove standalone apostrophes\n",
    "        text = re.sub(r'\\s\\'|\\'\\s', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_extra_whitespace(self, text):\n",
    "        \"\"\"Remove extra whitespace and newlines.\"\"\"\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        \"\"\"Lemmatize words to their root form.\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in words])\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove common stopwords.\"\"\"\n",
    "        words = text.split()\n",
    "        return ' '.join([word for word in words if word.lower() not in self.stop_words])\n",
    "    \n",
    "    def preprocess_text(self, text, steps=None):\n",
    "        if steps is None:\n",
    "            steps = ['expand_contractions', 'remove_special_chars', \n",
    "                    'remove_extra_whitespace', 'lemmatize_text', 'remove_stopwords']\n",
    "        \n",
    "        # Convert to lowercase first\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        for step in steps:\n",
    "            if hasattr(self, step):\n",
    "                text = getattr(self, step)(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df, column='lyrics', steps=None, inplace=False):\n",
    "\n",
    "        if not inplace:\n",
    "            df = df.copy()\n",
    "        \n",
    "        # Add preprocessed column\n",
    "        preprocessed_column = f'preprocessed_{column}'\n",
    "        df[preprocessed_column] = df[column].apply(lambda x: self.preprocess_text(x, steps))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    return LyricsPreprocessor().preprocess_dataframe(chorus_df)\n",
    "ch_processed = preprocess_df(chorus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test single subset df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.614614191000141\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     country       0.61      0.53      0.57      1316\n",
      "         pop       0.75      0.29      0.42       914\n",
      "         rap       0.68      0.79      0.74      1901\n",
      "          rb       0.53      0.75      0.62      1831\n",
      "        rock       0.64      0.46      0.54      1127\n",
      "\n",
      "    accuracy                           0.61      7089\n",
      "   macro avg       0.64      0.56      0.57      7089\n",
      "weighted avg       0.63      0.61      0.60      7089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import vstack\n",
    "def test(model, X_test, y_test):\n",
    "    # Step 6: Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_test, y_pred)*100, 2))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "   \n",
    "def tfidf_weighted_combined(df1, df2, text_column='lyrics', weight1=1.0, weight2=0.4):\n",
    "    # Fill missing values with empty strings and ensure all data is string\n",
    "    df1[text_column] = df1[text_column].fillna('').astype(str)\n",
    "    df2[text_column] = df2[text_column].fillna('').astype(str)\n",
    "\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1, 3), dtype=np.float32)\n",
    "\n",
    "    # Transform chorus and non-chorus parts separately\n",
    "    chorus_features = tfidf.fit_transform(df1[text_column]) * weight1\n",
    "    non_chorus_features = tfidf.transform(df2[text_column]) * weight2\n",
    "\n",
    "    # Combine the features\n",
    "    combined_features = vstack([chorus_features, non_chorus_features])\n",
    "\n",
    "    # Combine target labels\n",
    "    combined_labels = np.concatenate([df1['tag'].values, df2['tag'].values])\n",
    "\n",
    "    return combined_features, combined_labels, tfidf\n",
    "\n",
    "\n",
    "\n",
    "def extract_and_reduce_features(df1, df2, weight1=1.0, weight2=1.0):\n",
    "    # Get preprocessed features\n",
    "    X_combined, y_combined, tfidf = tfidf_weighted_combined(\n",
    "        df1, df2, weight1=weight1, weight2=weight2\n",
    "    )\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    svd = TruncatedSVD(n_components=400, random_state=42)\n",
    "    X_combined = svd.fit_transform(X_combined)\n",
    "    return X_combined, y_combined, tfidf, svd\n",
    "\n",
    "def train_weighted_pipeline(df1, df2, model, weight1=1.0, weight2=0.8, model_name='weighted_model'):\n",
    "    # Get preprocessed features\n",
    "    \n",
    "    X_reduced , y_combined, tfidf,svd = extract_and_reduce_features(df1,df2,weight1,weight2)\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reduced, y_combined, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', model)  # Classifier only\n",
    "    ])\n",
    "\n",
    "    # Train the classifier\n",
    "    pipeline.named_steps['classifier'].fit(X_train, y_train)\n",
    "\n",
    "    # Save the pipeline with tfidf and svd as additional components\n",
    "    dump({'pipeline': pipeline, 'tfidf': tfidf, 'svd': svd}, model_name + '_pipeline.joblib')\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "# X_reduced, y_combined, tfidf, svd = extract_features(chorus_df, non_chorus_df,weight1=0.9, weight2=0.6)\n",
    "# # Split into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_reduced, y_combined, test_size=0.1, random_state=42\n",
    "# )\n",
    "\n",
    "# test(load(\"w-rf-pip_pipeline.joblib\"),X_test, y_test)\n",
    "train_weighted_pipeline(chorus_df, non_chorus_df, RandomForestClassifier( n_jobs=-1), \n",
    "                     weight1=1.5, weight2=0.9, model_name=\"4k-svd4-w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory error loading word2vec\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'vector_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 193\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test, y_pred))\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_test, y_pred\n\u001b[0;32m--> 193\u001b[0m y_test, y_pred\u001b[38;5;241m=\u001b[39mcustom_weighted_final(chorus_df,non_chorus_df, RandomForestClassifier(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m    194\u001b[0m                       weight1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom-svd4-10k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "Cell \u001b[0;32mIn[4], line 160\u001b[0m, in \u001b[0;36mcustom_weighted_final\u001b[0;34m(df1, df2, model, weight1, weight2, model_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m FeatureExtractor()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Prepare combined features\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m f1,f2 \u001b[38;5;241m=\u001b[39m feature_extractor\u001b[38;5;241m.\u001b[39mprepare_combined_features(\n\u001b[1;32m    161\u001b[0m     df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m'\u001b[39m], df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m'\u001b[39m], weight1\u001b[38;5;241m=\u001b[39mweight1,weight2\u001b[38;5;241m=\u001b[39mweight2\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    163\u001b[0m F_unified \u001b[38;5;241m=\u001b[39m hstack([X_tfidf, f1,f2])\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Shape of combined features\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 120\u001b[0m, in \u001b[0;36mFeatureExtractor.prepare_combined_features\u001b[0;34m(self, chorus_texts, non_chorus_texts, weight1, weight2)\u001b[0m\n\u001b[1;32m    116\u001b[0m stats_styles_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(stats_styles_features)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Compute word embeddings\u001b[39;00m\n\u001b[1;32m    119\u001b[0m embeddings_chorus \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m--> 120\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding_features(text, weight1) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m chorus_texts]\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m embeddings_non_chorus \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    123\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding_features(text, weight2) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m non_chorus_texts]\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    125\u001b[0m combined_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((embeddings_chorus, embeddings_non_chorus))\n",
      "Cell \u001b[0;32mIn[4], line 120\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m stats_styles_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(stats_styles_features)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Compute word embeddings\u001b[39;00m\n\u001b[1;32m    119\u001b[0m embeddings_chorus \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m--> 120\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding_features(text, weight1) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m chorus_texts]\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m embeddings_non_chorus \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    123\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding_features(text, weight2) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m non_chorus_texts]\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    125\u001b[0m combined_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((embeddings_chorus, embeddings_non_chorus))\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mFeatureExtractor.get_embedding_features\u001b[0;34m(self, text, weight)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mCompute average word embedding for the text.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_model:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_model\u001b[38;5;241m.\u001b[39mvector_size  \u001b[38;5;66;03m# Placeholder if no model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_model]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m words:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'vector_size'"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import string\n",
    "import gensim.downloader as api\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from gensim.downloader import load as gensim_load\n",
    "# import spacy  # Uncomment if spaCy is used\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sparse Matrix Operations\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from joblib import dump, load\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureExtractor with an optional word embedding model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.word_embedding_model = api.load('word2vec-google-news-300')\n",
    "        except:\n",
    "            print(\"Memory error loading word2vec\")\n",
    "            self.word_embedding_model = None\n",
    "    def _count_repeated_lines(self, text):\n",
    "        \"\"\"\n",
    "        Count repeated lines in the text.\n",
    "        \"\"\"\n",
    "        lines = text.split(\"\\n\")\n",
    "        return len(lines) - len(set(lines))\n",
    "\n",
    "    def get_stylistic_features(self, text):\n",
    "        \"\"\"\n",
    "        Generate stylistic features from the text.\n",
    "        \"\"\"\n",
    "        exclamation_count = text.count(\"!\")\n",
    "        question_count = text.count(\"?\")\n",
    "        uppercase_ratio = sum(1 for c in text if c.isupper()) / len(text) if text else 0\n",
    "        digit_ratio = sum(1 for c in text if c.isdigit()) / len(text) if text else 0\n",
    "        punctuation_ratio = (\n",
    "            sum(1 for c in text if c in string.punctuation) / len(text) if text else 0\n",
    "        )\n",
    "        repeated_lines = self._count_repeated_lines(text)\n",
    "        return [\n",
    "            exclamation_count,\n",
    "            question_count,\n",
    "            uppercase_ratio,\n",
    "            digit_ratio,\n",
    "            punctuation_ratio,\n",
    "            repeated_lines,\n",
    "        ]\n",
    "\n",
    "    def get_statistical_features(self, text):\n",
    "        \"\"\"\n",
    "        Extract statistical features from text.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        unique_word_count = len(set(words))\n",
    "        avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "        return [word_count, unique_word_count, avg_word_length]\n",
    "\n",
    "    def get_embedding_features(self, text, weight=1.0):\n",
    "        \"\"\"\n",
    "        Compute average word embedding for the text.\n",
    "        \"\"\"\n",
    "        if not self.word_embedding_model:\n",
    "            return [0] * self.word_embedding_model.vector_size  # Placeholder if no model\n",
    "        words = [word for word in text.split() if word in self.word_embedding_model]\n",
    "        if not words:\n",
    "            return [0] * self.word_embedding_model.vector_size\n",
    "        return np.mean([self.word_embedding_model[word] for word in words], axis=0) * weight\n",
    "\n",
    "    def combine_statistical_and_stylistic_features(self, chorus_texts, non_chorus_texts, weight1=1.5, weight2=1.0):\n",
    "        \"\"\"\n",
    "        Combine statistical and stylistic features with weighted chorus features.\n",
    "        \"\"\"\n",
    "        def extract_features(texts):\n",
    "            stats = [self.get_statistical_features(text) for text in texts]\n",
    "            styles = [self.get_stylistic_features(text) for text in texts]\n",
    "            return np.hstack((stats, styles))\n",
    "\n",
    "        # Extract and weight features\n",
    "        chorus_features = extract_features(chorus_texts) * weight1\n",
    "        non_chorus_features = extract_features(non_chorus_texts) * weight2\n",
    "\n",
    "        # Combine and normalize features\n",
    "        combined_features = np.vstack((chorus_features, non_chorus_features))\n",
    "        normalized_features = MinMaxScaler().fit_transform(combined_features)\n",
    "        if chorus_features.size == 0 or non_chorus_features.size == 0:\n",
    "            raise ValueError(\"Features for chorus or non-chorus texts are missing.\")\n",
    "\n",
    "        return normalized_features\n",
    "\n",
    "    def prepare_combined_features(self, chorus_texts, non_chorus_texts, weight1=1.5, weight2=1.0):\n",
    "        \"\"\"\n",
    "        Combine TF-IDF, statistical, stylistic, and word embedding features.\n",
    "        \"\"\"\n",
    "        # Combine statistical and stylistic features\n",
    "        stats_styles_features = self.combine_statistical_and_stylistic_features(\n",
    "            chorus_texts, non_chorus_texts, weight1, weight2\n",
    "        )\n",
    "        stats_styles_sparse = csr_matrix(stats_styles_features)\n",
    "\n",
    "        # Compute word embeddings\n",
    "        embeddings_chorus = np.array(\n",
    "            [self.get_embedding_features(text, weight1) for text in chorus_texts]\n",
    "        )\n",
    "        embeddings_non_chorus = np.array(\n",
    "            [self.get_embedding_features(text, weight2) for text in non_chorus_texts]\n",
    "        )\n",
    "        combined_embeddings = np.vstack((embeddings_chorus, embeddings_non_chorus))\n",
    "        embeddings_sparse = csr_matrix(combined_embeddings)\n",
    "\n",
    "        # Combine all features\n",
    "        return stats_styles_sparse, embeddings_sparse\n",
    "\n",
    "        \n",
    "def tfidf_weighted_combined(df1, df2, text_column='lyrics', weight1=1.0, weight2=0.4):\n",
    "    # Fill missing values with empty strings and ensure all data is string\n",
    "    df1[text_column] = df1[text_column].fillna('').astype(str)\n",
    "    df2[text_column] = df2[text_column].fillna('').astype(str)\n",
    "\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1, 3), dtype=np.float32)\n",
    "\n",
    "    # Transform chorus and non-chorus parts separately\n",
    "    chorus_features = tfidf.fit_transform(df1[text_column]) * weight1\n",
    "    non_chorus_features = tfidf.transform(df2[text_column]) * weight2\n",
    "\n",
    "    # Combine the features\n",
    "    combined_features = vstack([chorus_features, non_chorus_features])\n",
    "\n",
    "    # Combine target labels\n",
    "    combined_labels = np.concatenate([df1['tag'].values, df2['tag'].values])\n",
    "\n",
    "    return combined_features, combined_labels, tfidf\n",
    "\n",
    "def custom_weighted_final(df1, df2, model, weight1=1.0, weight2=1.0, model_name=\"c-w\"):\n",
    "    # Example TF-IDF features (replace with actual computation)\n",
    "    X_tfidf, y_combined, tfidf = tfidf_weighted_combined(\n",
    "            df1, df2, weight1=weight1, weight2=weight2)\n",
    "\n",
    "    # Initialize feature extractor with a word embedding model\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    # Prepare combined features\n",
    "    f1,f2 = feature_extractor.prepare_combined_features(\n",
    "        df1['lyrics'], df2['lyrics'], weight1=weight1,weight2=weight2\n",
    "    )\n",
    "    F_unified = hstack([X_tfidf, f1,f2])\n",
    "    \n",
    "    # Shape of combined features\n",
    "    print(\"Combined feature shape:\", F_unified.shape)\n",
    "    svd = TruncatedSVD(n_components=400, random_state=42)\n",
    "    F_unified = svd.fit_transform(F_unified)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        F_unified, y_combined, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', model)  # Classifier only\n",
    "    ])\n",
    "\n",
    "    # Train the classifier\n",
    "    pipeline.named_steps['classifier'].fit(X_train, y_train)\n",
    "\n",
    "    # Save the pipeline with tfidf and svd as additional components\n",
    "    dump({'pipeline': pipeline, 'tfidf': tfidf, 'svd': svd, \n",
    "        'feature_extractor': feature_extractor,\n",
    "          }, model_name + '_pipeline.joblib')\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    return y_test, y_pred\n",
    "\n",
    "y_test, y_pred=custom_weighted_final(chorus_df,non_chorus_df, RandomForestClassifier(n_jobs=-1), \n",
    "                      weight1=0.9, weight2=0.6, model_name=\"custom-svd4-10k\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nb_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
